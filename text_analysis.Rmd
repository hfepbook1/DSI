---
title: "Text analysis"
output:
  html_document: 
    toc: true
    toc_float: true
---

A bit of a digression for a DS class in a Biostat department, but it's fun, easily related to web data, and deals with strings and factors. 

This is the third module in the [Data Wrangling II](topic_data_wrangling_ii.html) topic; the relevant slack channel is [here](https://p8105-fall2017.slack.com/messages/C7N8HG34N).

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```


## Some slides

<script async class="speakerdeck-embed" data-id="bfb454627b71408889958f0893413097" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>
<div style="margin-bottom:5px"> <strong> <a href="https://speakerdeck.com/jeffgoldsmith/dsi-strings-and-factors" title="Strings and Factors" target="_blank">Strings and Factors</a> </strong> from <strong><a href="https://speakerdeck.com/jeffgoldsmith" target="_blank">Jeff Goldsmith</a></strong>. </div><br>


## Example

https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html

```{r}
library(tidyverse)
library(tidytext)
library(stringr)
library(forcats)

library(viridis)
```

### Data

```{r}
nyc_inspections = read_csv("./data/DOHMH_New_York_City_Restaurant_Inspection_Results.csv.gz", 
                           col_types = cols(building = col_character()),
                           na = c("NA", "N/A")) %>% 
  filter(grade %in% c("A", "B", "C")) %>% 
  mutate(inspection_num = row_number(),
         boro = str_to_title(boro)) %>% 
  select(inspection_num, boro, grade, score, critical_flag, dba, cuisine_description, zipcode, violation_description)
```

### Words and wordcounts

```{r}
data(stop_words)
```


```{r}
restaurant_words = nyc_inspections %>% 
  unnest_tokens(word, violation_description) %>% 
  anti_join(stop_words, by = "word")
```


```{r}
word_counts = restaurant_words %>% count(word, sort = TRUE)

word_counts %>% 
  top_n(10) %>% 
  mutate(word = fct_reorder(word, n)) %>% 
  ggplot(aes(x = word, y = n)) + 
  geom_bar(stat = "identity", fill = "#009E73") + 
  coord_flip()
```

### Comparing words across groups

Finally, let’s compare which words are more likely to come from a “C” versus “A” inspection. We limit to words that appear at least 3 times, and compute the log odds ratio for each word, for the odds of the word appearing in a “C” inspection to the odds of the word appearing in an “A” inspection.

```{r}
word_ratios <- restaurant_words %>%
    filter(grade %in% c("A", "C")) %>% 
    count(word, grade) %>%
    group_by(word) %>% 
    filter(sum(n) >= 5) %>%
    ungroup() %>%
    spread(grade, n, fill = 0) %>%
    mutate_if(is.numeric, funs((. + 1) / sum(. + 1))) %>%
    mutate(logratio = log(C / A)) %>%
    arrange(desc(logratio)) 
```

We plot the top 15 most distinct words that are more common for “C” and “A” graded inspections.

```{r}
word_ratios %>%
  group_by(logratio < 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_col() +
  coord_flip() +
  ylab("log odds ratio (C/A)") +
  scale_fill_discrete(name = "", labels = c("C", "A"))
```



### Sentiment analysis

Now lets score the sentiment in each word. Note that only words that are in the selected sentiment lexicon will be retained, as the rest of the words are not considered meaningful. Let’s start with the sentiment lexicon “bing”, which simply categorizes each word as having a positive or negative sentiment. Then we’ll count the number of positive and negative words in each violation description, and create a score that represents the difference in the number of positive words minus the number of negative words.

```{r}
bing_sentiments = get_sentiments("bing")
```

Not perfect (e.g. this scores `cold` as `negative` which might not be accurate), but we'll use it. 

```{r}
restaurant_word_sentiments = restaurant_words %>% 
  inner_join(., bing_sentiments) %>% 
  count(inspection_num, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  select(inspection_num, sentiment)
```



```{r}
inspection_sentiments = 
  right_join(nyc_inspections, restaurant_word_sentiments, 
             by = "inspection_num")

inspection_sentiments %>% 
  filter(sentiment < -5) %>% 
  select(violation_description) %>% 
  print(n = 2)
```


```{r}
inspection_sentiments %>% 
  filter(boro == "Manhattan") %>% 
  mutate(inspection_num = factor(inspection_num),
    inspection_num = fct_reorder(inspection_num, sentiment)) %>% 
  ggplot(aes(x = inspection_num, 
             y = sentiment, fill = grade, color = grade)) + 
  geom_bar(stat = "identity") + 
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE) + 
  scale_color_viridis(discrete = TRUE) 
```




## Other materials

* https://www.youtube.com/watch?v=0poJP8WQxew
* http://tidytextmining.com
* http://varianceexplained.org/r/trump-tweets/



The code that I produced working examples in lecture is [here]().
