---
title: "Reading data from the web"
output:
  html_document: 
    toc: true
    toc_float: true
---

In [data import](data_import.html), we saw how to load data from a variety of formats; this is a fairly standard way to get data that have been gathered as part of a study. In a lot of cases, though, you're going to have to go out and get the data you want or need. That's the case we're covering now.

This is the first module in the [Data Wrangling II](topic_data_wrangling_ii.html) topic; the relevant slack channel is [here](ZZZZZZ).

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

## Some slides

<script async class="speakerdeck-embed" data-id="4e6375cc48814bd2b43b667180dbb31a" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>
<div style="margin-bottom:5px"> <strong> <a href="https://speakerdeck.com/jeffgoldsmith/dsi-data-manipulation" title="Data Manipulation" target="_blank">Data Manipulation</a> </strong> from <strong><a href="https://speakerdeck.com/jeffgoldsmith" target="_blank">Jeff Goldsmith</a></strong>. </div><br>


## Example

As always, it's good to start by figuring out how you want to organize you code for this content. I'll create a new directory called `data_wrangling_ii`, start an R Project, and open a new R Markdown file called `reading_data_from_the_web.Rmd`. Although we'll mostly be getting data from the web, we'll revisit some of [these examples](./resources/data_import_examples.zip), so I'll create a `data` subdirectory and put those in it.

There are some new additions to our standard packages (`rvest` and `httr`); I'm loading everything we need now. Now's also the time to install the [Selector Gadget](http://selectorgadget.com). 

```{r}
library(tidyverse)
library(rvest)
library(httr)
```

### Extracting tables

[This page](http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm) contains data from the National Survey on Drug Use and Health; it includes tables for drug use in the past year or month, separately for specific kinds of drug use. These data are potentially useful for analysis, and we'd like to be able to read in the first table. 

First, let's make sure we can load the data from the web. 

```{r}
url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"
drug_use_xml = read_html(url)

drug_use_xml
```

Doesn't look like much, but we're there. Rather than trying to grab something using a CSS selector, let's try our luck extracting the tables from the HTML.

```{r}
drug_use_xml %>%
  html_nodes(css = "table")
```

This has extracted _all_ of the tables on the original page; that's why we have a list with 15 elements. We're only focused on the first one for now, so let's get the contents from the first list element.

```{r}
table_marj = (drug_use_xml %>% html_nodes(css = "table"))[[1]] %>%
  html_table() 
```

I won't print the table here, but if you look at it you'll notice a problem: the "note" at the bottom of the table appears in every column in the first row. We need to remove that; I'll also convert to a tibble so that things print nicely.

```{r}
table_marj = (drug_use_xml %>% html_nodes(css = "table"))[[1]] %>%
  html_table() %>%
  .[-1,] %>% 
  as_tibble()

table_marj
```

Success!! At least, mostly. These data aren't [tidy](tidy_data.html), but we'll worry about that in another module. 

**_Learning assessment:_** Create a data frame that contains the cost of living table for New York from [this page](https://www.bestplaces.net/cost_of_living/city/new_york/new_york).

```{r, eval = FALSE, echo =  FALSE}
nyc_cost = read_html("https://www.bestplaces.net/cost_of_living/city/new_york/new_york") %>%
  html_nodes(css = "table") %>%
  .[[2]] %>%
  html_table(header = TRUE)
```

### CSS Selectors

Suppose we'd like to scrape the cast of [Harry Potter and the Sorcerer's Stone](http://www.imdb.com/title/tt0241527/) from the IMDB page. The first step is the same as before -- we need to get the HTML.

```{r}
hpss_html = read_html("http://www.imdb.com/title/tt0241527/")
```

The cast list isn't stored in a handy table, so we're going to isolate the CSS selector for cast. A bit of clicking around gets me something like below. 

<img src="images/read_from_web_css_selctor.png" style="width:75%">

I'll use the CSS selector in `html_nodes()` to extract the relevant HTML code, and convert it to text. 

```{r}
cast = hpss_html %>%
  html_nodes(".itemprop .itemprop") %>%
  html_text()
```

**_Learning Assessment:_** [This page](https://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/product-reviews/0387981403/ref=cm_cr_arp_d_hist_5?ie=UTF8&showViewpoints=1&sortBy=helpful&filterByStar=five_star&pageNumber=1) contains the five-star reviews of a book on ggplot2. Use a process similar to the one above to extract the titles of the reviews. 

```{r, eval = FALSE, echo = FALSE}
url = "https://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/product-reviews/0387981403/ref=cm_cr_arp_d_hist_5?ie=UTF8&showViewpoints=1&sortBy=helpful&filterByStar=five_star&pageNumber=1"

ggplot2_book_html = read_html(url)

review_titles = ggplot2_book_html %>%
  html_nodes(".a-color-base") %>%
  html_text()
```


### Using an API

New York City has a great open data resource. Although most (all?) datasets can be accessed by clicking through a website, we'll access them directly using the API to improve reproducibility.

As a simple example, [this page](https://data.cityofnewyork.us/Environment/Water-Consumption-In-The-New-York-City/ia2d-e54m) is about a dataset for annual water consumption in NYC, along with the population in that year. 

```{r}
nyc_water = GET("https://data.cityofnewyork.us/resource/waf7-5gvc.csv") %>% 
  content("parsed")

nyc_water = GET("https://data.cityofnewyork.us/resource/waf7-5gvc.json") %>% 
  content("text") %>%
  jsonlite::fromJSON() %>%
  as_tibble()
```


```{r}
nyc_water = 
  GET("https://data.cityofnewyork.us/resource/waf7-5gvc.csv",
        query = list(year = 1990)
      ) %>% 
  content("parsed")
```


**_Learning Assessment:_** Using information on [this page](https://data.cityofnewyork.us/resource/hvwh-qtfg), get the information on subway entrances in the city. Which stations have the most entrances?

```{r, eval = FALSE, echo = FALSE}
subway_entrances = 
  GET("https://data.ny.gov/resource/hvwh-qtfg.csv") %>%
  content("parsed") %>% 
  group_by(station_name) %>% 
  summarize(n_enter = n()) %>% 
  arrange(desc(n_enter))
```

The NYC open data is particularly user-friendly; not every API is, and many take some work to get used to. Let's walk through an example from the Star Wars API.

```{r}

req = GET("http://swapi.co/api/people")
con = content(req)
con$results[[1]]





## http://pokeapi.co/docsv2/#pokemon

poke = GET("http://pokeapi.co/api/v2/pokemon")
con = content(poke)
con$results[[1]]


poke = GET("http://pokeapi.co/api/v2/pokemon/1")
con = content(poke)
con$stats[[1]]



get_all <- function(url) {
  out <- NULL
  
  while(!is.null(url)) {
    message("Getting ", url)
    req <- GET(url)
    stop_for_status(req)
    
    con <- content(req)
    out <- c(out, con$results)
    url <- con$`next`
  }
  
  out
}

# people <- get_all("http://swapi.co/api/people")
# str(people[[1]])

# poke = get_all("http://pokeapi.co/api/v2/pokemon")
```

Lastly, there are R packages for several popular APIs, including the `rnoaa` package we used in [visualization](visualization.html) and the `twitteR` package for interacting with twitter. 

## Other materials

https://www.opendatanetwork.com/search?q=new+york+city

https://github.com/ropensci/user2016-tutorial?utm_content=buffer604f6&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer

https://community.rstudio.com/t/whats-the-most-interesting-use-of-rvest-youve-seen-in-the-wild/745

https://github.com/ropensci/user2016-tutorial

https://github.com/tidyverse/dplyr/blob/master/data-raw/starwars.R

https://github.com/datasciencelabs/2016/blob/master/lectures/wrangling/rvest-scraping.Rmd

https://github.com/datasciencelabs/2016/blob/master/lectures/wrangling/httr_twitter_and_text.Rmd

https://zapier.com/learn/apis/

https://dev.socrata.com/foundry/data.cityofnewyork.us/9w7m-hzhe


The code that I produced working examples in lecture is [here](./lecture_code/data_wrangling_ii.zip).
